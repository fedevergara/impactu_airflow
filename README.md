# ImpactU Airflow ETL

Repositorio central de DAGs de Apache Airflow para los procesos de Extracci贸n, Transformaci贸n y Carga (ETL) del proyecto ImpactU.

##  Descripci贸n
Este proyecto orquestaliza la recolecci贸n de datos de diversas fuentes cient铆ficas y acad茅micas, su procesamiento mediante la herramienta [Kahi](https://github.com/colav/Kahi) y su posterior carga en sistemas de consulta como MongoDB y Elasticsearch.

##  Estructura del Proyecto
El repositorio est谩 organizado por etapas del ciclo de vida del dato:

*   `extract/`: L贸gica de extracci贸n para fuentes como OpenAlex, ORCID, ROR, etc.
*   `transform/`: Procesos de transformaci贸n y normalizaci贸n (Kahi).
*   `load/`: Scripts de carga hacia destinos finales.
*   `deploys/`: Configuraciones de despliegue por entorno (dev, prod).
*   `backups/`: Automatizaci贸n de respaldos de bases de datos.
*   `tests/`: Pruebas de integraci贸n y calidad de datos.

##  Requisitos y Arquitectura
Para detalles sobre los principios de dise帽o (Checkpoints, Idempotencia, Paralelismo), consulte el documento de [Requisitos del Sistema](REQUISITOS.md).

##  Est谩ndar de Nombrado de DAGs
Para mantener la consistencia en la interfaz de Airflow, seguimos esta convenci贸n:

| Tipo | Formato | Ejemplo |
| :--- | :--- | :--- |
| **Extracci贸n** | `extract_{fuente}` | `extract_openalex` |
| **Transformaci贸n** | `transform_{entidad}` | `transform_sources` |
| **Carga** | `load_{db}_{env}` | `load_mongodb_production` |
| **Despliegue** | `deploy_{servicio}_{env}` | `deploy_mongodb_production` |
| **Backup** | `backup_{db}_{nombre}` | `backup_mongodb_kahi` |
| **Pruebas** | `tests_{servicio}` | `tests_kahi` |

## 锔 Configuraci贸n y Desarrollo
*(Secci贸n en construcci贸n)*

### Requisitos Previos
*   Docker & Docker Compose
*   Apache Airflow 3.1.5
*   Python 3.12+

### Instalaci贸n
1. Clonar el repositorio.
2. Configurar las variables de entorno en un archivo `.env`.
3. Levantar el entorno con Docker Compose.

---
**Colav - ImpactU**
